---
import BaseLayout from "../../../layouts/BaseLayout.astro";
---

<BaseLayout title = "Point cloud painting" sideBarActiveItemID="publications">
    <div class="flex justify-start mb-4">
      <a href="/publications" class="inline-flex items-center px-4 py-2 text-base-content rounded-lg hover:underline focus:underline transition-colors">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2" viewBox="0 0 20 20" fill="currentColor">
          <path fill-rule="evenodd" d="M7.707 14.707a1 1 0 01-1.414 0l-5-5a1 1 0 010-1.414l5-5a1 1 0 011.414 1.414L4.414 9H17a1 1 0 110 2H4.414l3.293 3.293a1 1 0 010 1.414z" clip-rule="evenodd" />
        </svg>
        Back
      </a>
    </div>
    
    <div>
        <div class="text-3xl w-full font-bold mb-5">
            Point cloud painting for 3D object detection with camera and automotive 3+ 1D RADAR fusion
        </div>
    </div>

    <div class="mt-8 mb-8 p-6 bg-base-200 rounded-xl shadow-lg">
      <h2 class="text-2xl font-bold mb-4">Links</h2>
      
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
        <!-- DOI -->
        <a href="https://doi.org/10.3390/s24041244" class="flex items-center gap-2 hover:text-accent transition-colors text-base-content">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" fill="currentColor">
            <path fill="currentColor" d="M115.592 222.507c-9.726 0-18.734 7.074-24.986 14.417c-5.06 5.954-7.59 15.133-7.59 22.775c0 7.64 2.87 16.139 7.931 22.093c6.152 7.343 12.778 12.375 22.603 12.375c8.833 0 19.338-3.708 25.39-9.76c6.151-6.054 11.269-16.218 11.269-25.05c0-8.73-2.396-19.186-8.548-25.24c-6.153-6.154-17.338-11.61-26.07-11.61zm174.644 2.4c-10.12 0-18.358 3.98-24.709 11.937c-5.158 6.386-7.74 13.663-7.74 21.828c0 8.271 2.582 15.6 7.74 21.986c6.35 7.956 14.589 11.937 24.71 11.937c9.03 0 16.67-3.251 22.92-9.742c6.254-6.597 9.379-14.653 9.379-24.18c0-9.423-3.177-17.377-9.527-23.868c-6.25-6.595-13.843-9.898-22.773-9.898zM256 8C119.034 8 8 119.034 8 256s111.034 248 248 248s248-111.034 248-248S392.966 8 256 8Zm-58.826 323.79H151.24v-18.699c-4.665 5.855-15.639 11.824-20.4 14.502c-8.335 4.663-19.654 7.7-30.273 7.7c-17.168 0-32.45-5.424-45.845-17.231c-15.977-14.09-23.483-35.633-23.483-58.854c0-23.618 8.186-42.473 24.56-56.563c12.999-11.214 28.896-20.19 45.766-20.19c9.822 0 21.94.883 30.572 5.05c4.962 2.38 13.844 7.985 19.104 13.443v-98.502h45.936v229.345zm157.216-21.14c-15.281 17.588-36.667 26.384-64.154 26.384c-27.586 0-49.02-8.796-64.304-26.383c-12.6-14.448-18.904-32.193-18.904-53.235c0-18.95 6.35-35.752 19.053-50.408c15.38-17.693 37.261-26.54 65.64-26.54c26.099 0 46.937 8.845 62.518 26.54c12.702 14.448 19.052 31.667 19.052 51.664c.003 20.206-6.299 37.531-18.901 51.98zm38.769-199.217c5.359-5.36 11.858-8.038 19.499-8.038c7.64 0 14.14 2.679 19.499 8.038c5.358 5.26 8.037 11.71 8.037 19.35c0 7.74-2.679 14.289-8.037 19.648-5.26 5.259-11.76 7.888-19.5 7.888c-7.642 0-14.142-2.679-19.498-8.038c-5.26-5.358-7.889-11.858-7.889-19.499c-.003-7.64 2.63-14.09 7.889-19.35zm43.91 220.242h-48.823V185.841h48.823z"/>
              <span>10.3390/s24041244</span>
        </a>
        <a href="https://www.mdpi.com/1424-8220/24/4/1244" class="flex items-center gap-2 hover:text-accent transition-colors">
            <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="inline ml-2 hover:text-accent transition-colors">
                <path d="M13.213 9.787a3.391 3.391 0 0 0-4.795 0l-3.425 3.426a3.39 3.39 0 0 0 4.795 4.794l.321-.304m-.321-4.49a3.39 3.39 0 0 0 4.795 0l3.424-3.426a3.39 3.39 0 0 0-4.794-4.795l-1.028.961"/>
            </svg>
          <span>https://www.mdpi.com/1424-8220/24/4/1244</span>
        </a>
        </div>
    </div>

    <p>
        RADARs and cameras have been present in automotives since the advent of ADAS, 
        as they possess complementary strengths and weaknesses but have been underlooked
        in the context of learning-based methods. In this work, we propose a method to 
        perform object detection in autonomous driving based on a geometrical and sequential 
        sensor fusion of 3+1D RADAR and semantics extracted from camera data through point 
        cloud painting from the perspective view. To achieve this objective, we adapt 
        PointPainting from the LiDAR and camera domains to the sensors mentioned above. 
        We first apply YOLOv8-seg to obtain instance segmentation masks and project their 
        results to the point cloud. As a refinement stage, we design a set of heuristic rules 
        to minimize the propagation of errors from the segmentation to the detection stage.
        Our pipeline concludes by applying PointPillars as an object detection network to the 
        painted RADAR point cloud. We validate our approach in the novel View of 
        Delft dataset, which includes 3+1D RADAR data sequences in urban environments. 
        Experimental results show that this fusion is also suitable for RADAR and cameras 
        as we obtain a significant improvement over the RADAR-only baseline, increasing mAP 
        from 41.18 to 52.67 (+27.9%).

        <div class="flex justify-center mt-4 mb-4">
            <img src="https://www.mdpi.com/sensors/sensors-24-01244/article_deploy/html/images/sensors-24-01244-g002-550.jpg" 
                 alt="Pipeline Architecture" 
                 class="max-w-full h-auto rounded-lg"/>
        </div>
    </p>

</BaseLayout>
  