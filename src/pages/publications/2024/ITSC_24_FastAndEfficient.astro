---
import BaseLayout from "../../../layouts/BaseLayout.astro";
---

<BaseLayout title = "ITSC 2024" sideBarActiveItemID="publications">

    <div class="flex justify-start mb-4">
      <a href="/publications" class="inline-flex items-center px-4 py-2 text-base-content rounded-lg hover:underline focus:underline transition-colors">
        <svg xmlns="http://www.w3.org/2000/svg" class="h-5 w-5 mr-2" viewBox="0 0 20 20" fill="currentColor">
          <path fill-rule="evenodd" d="M7.707 14.707a1 1 0 01-1.414 0l-5-5a1 1 0 010-1.414l5-5a1 1 0 011.414 1.414L4.414 9H17a1 1 0 110 2H4.414l3.293 3.293a1 1 0 010 1.414z" clip-rule="evenodd" />
        </svg>
        Back
      </a>
    </div>
    <div>
        <div class="text-3xl w-full font-bold mb-5">
            Fast and Efficient Transformer-based Method for Bird's Eye View Instance Prediction
        </div>
    </div>

    <div class="mt-8 mb-8 p-6 bg-base-200 rounded-xl shadow-lg">
      <h2 class="text-2xl font-bold mb-4">Links</h2>
      
      <div class="grid grid-cols-1 md:grid-cols-2 gap-4">
        <!-- DOI -->
    <a href="https://doi.org/10.1109/ITSC58415.2024.10919912" class="flex items-center gap-2 hover:text-accent transition-colors text-base-content">
      <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 512 512" fill="currentColor">
        <path fill="currentColor" d="M115.592 222.507c-9.726 0-18.734 7.074-24.986 14.417c-5.06 5.954-7.59 15.133-7.59 22.775c0 7.64 2.87 16.139 7.931 22.093c6.152 7.343 12.778 12.375 22.603 12.375c8.833 0 19.338-3.708 25.39-9.76c6.151-6.054 11.269-16.218 11.269-25.05c0-8.73-2.396-19.186-8.548-25.24c-6.153-6.154-17.338-11.61-26.07-11.61zm174.644 2.4c-10.12 0-18.358 3.98-24.709 11.937c-5.158 6.386-7.74 13.663-7.74 21.828c0 8.271 2.582 15.6 7.74 21.986c6.35 7.956 14.589 11.937 24.71 11.937c9.03 0 16.67-3.251 22.92-9.742c6.254-6.597 9.379-14.653 9.379-24.18c0-9.423-3.177-17.377-9.527-23.868c-6.25-6.595-13.843-9.898-22.773-9.898zM256 8C119.034 8 8 119.034 8 256s111.034 248 248 248s248-111.034 248-248S392.966 8 256 8Zm-58.826 323.79H151.24v-18.699c-4.665 5.855-15.639 11.824-20.4 14.502c-8.335 4.663-19.654 7.7-30.273 7.7c-17.168 0-32.45-5.424-45.845-17.231c-15.977-14.09-23.483-35.633-23.483-58.854c0-23.618 8.186-42.473 24.56-56.563c12.999-11.214 28.896-20.19 45.766-20.19c9.822 0 21.94.883 30.572 5.05c4.962 2.38 13.844 7.985 19.104 13.443v-98.502h45.936v229.345zm157.216-21.14c-15.281 17.588-36.667 26.384-64.154 26.384c-27.586 0-49.02-8.796-64.304-26.383c-12.6-14.448-18.904-32.193-18.904-53.235c0-18.95 6.35-35.752 19.053-50.408c15.38-17.693 37.261-26.54 65.64-26.54c26.099 0 46.937 8.845 62.518 26.54c12.702 14.448 19.052 31.667 19.052 51.664c.003 20.206-6.299 37.531-18.901 51.98zm38.769-199.217c5.359-5.36 11.858-8.038 19.499-8.038c7.64 0 14.14 2.679 19.499 8.038c5.358 5.26 8.037 11.71 8.037 19.35c0 7.74-2.679 14.289-8.037 19.648-5.26 5.259-11.76 7.888-19.5 7.888c-7.642 0-14.142-2.679-19.498-8.038c-5.26-5.358-7.889-11.858-7.889-19.499c-.003-7.64 2.63-14.09 7.889-19.35zm43.91 220.242h-48.823V185.841h48.823z"/>
          <span>10.1109/ITSC58415.2024.10919912</span>
        </a>
        <a href="https://github.com/miguelag99/Efficient-Instance-Prediction" class="flex items-center gap-2 hover:text-accent transition-colors">
          <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round">
            <path d="M15 22v-4a4.8 4.8 0 0 0-1-3.5c3 0 6-2 6-5.5.08-1.25-.27-2.48-1-3.5.28-1.15.28-2.35 0-3.5 0 0-1 0-3 1.5-2.64-.5-5.36-.5-8 0C6 2 5 2 5 2c-.3 1.15-.3 2.35 0 3.5A5.403 5.403 0 0 0 4 9c0 3.5 3 5.5 6 5.5-.39.49-.68 1.05-.85 1.65-.17.6-.22 1.23-.15 1.85v4"/>
            <path d="M9 18c-4.51 2-5-2-7-2"/>
          </svg>
          <span>miguelag99/Efficient-Instance-Prediction</span>
        </a>
        </div>
    </div>

    <p>
        Accurate object detection and prediction are critical to ensure the safety and efficiency of self-driving architectures.
        Predicting object trajectories and occupancy enables autonomous vehicles to anticipate movements and make decisions with future
        information, increasing their adaptability and reducing the risk of accidents. Current State-Of-The-Art (SOTA) approaches often
        isolate the detection, tracking, and prediction stages, which can lead to significant prediction errors due to accumulated inaccuracies
        between stages. Recent advances have improved the feature representation of multi-camera perception systems through
        Bird's-Eye View (BEV) transformations, boosting the development of end-to-end systems capable of predicting environmental elements
        directly from vehicle sensor data. These systems, however, often suffer from high processing times and number of parameters,
        creating challenges for real-world deployment. To address these issues, this paper introduces a novel BEV instance prediction
        architecture based on a simplified paradigm that relies only on instance segmentation and flow prediction.
        The proposed system prioritizes speed, aiming at reduced parameter counts and inference times compared to existing
        SOTA architectures, thanks to the incorporation of an efficient transformer-based architecture.
        Furthermore, the implementation of the proposed architecture is optimized for performance improvements in PyTorch version 2.1.


        <div class="flex justify-center mt-4 mb-4">
            <img src="https://raw.githubusercontent.com/miguelag99/Efficient-Instance-Prediction/7340c4cc06ffd901f0a2373dfa6b9f223a9a136c/docs/results.png" 
                 alt="Pipeline Architecture" 
                 class="max-w-full h-auto shadow-lg rounded-lg"/>
        </div>
    </p>


</BaseLayout>
  
